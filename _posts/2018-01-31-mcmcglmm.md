---
title: "Meta-analysis for biologists using MCMCglmm"
author: "Kat"
date: "2018-01-31 14:00:00"
meta: Tutorials
subtitle: "An introduction to the MCMCglmm package"
layout: post
tags: github modelling
---
<div class="block">
	<center>
		<img src="{{ site.baseurl }}/img/tutheader_tutname.png" alt="Img">
	</center>
</div>


This tutorial is aimed at people who are new to meta-analysis and using **MCMCglmm**, to help you become comfortable with using the package, and learn some of the ways you can analyse your data. It isn’t designed to teach you about hardcore Bayesian statistics or mixed modelling, but rather to highlight the differences between MCMCglmm and other statistical approaches you may have used before, and overcome some of the problems you may encounter as a new user. You'll find the resources for the tutorial __<a href="https://github.com/ourcodingclub/CC-MCMCglmm-MA" target="_blank">in this repository</a>__.

Don't worry if you leave this tutorial feeling like you haven't grasped key concepts. It will probably take practice and time to get to grips with all of this!

First we will learn a little about how MCMCglmm works. Then we’ll explore some **syntax**, get a **model** up and running, learn how to make sure it has run correctly and interpret the results. Then we'll move on to things that are a little more complicated, and __<a href="https://www.youtube.com/watch?v=o7lilfpZNGc" target="_blank">this might happen</a>__.

Before we begin, you may like to check out the following links, which will help you understand a bit more about Bayesian statistics so you can hit the ground running in the tutorial.

__<a href="https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/" target="_blank">This link</a>__ is gives a quick description of the difference between **Bayesian** and **frequentist** statistics. __<a href="https://www.theregister.co.uk/2017/06/22/bayesian_vs_frequentist_ai/" target="_blank">This link</a>__  does too, and is about zombies. You don't need to become a Bayesian statistician to use MCMCglmm, but it will be __VERY USEFUL__ to understand the difference when thinking about how the package works. If you are really into learning more __[these YouTube tutorials](https://www.youtube.com/watch?v=U1HbB0ATZ_A&index=1&list=PLFDbGp5YzjqXQ4oE4w9GVWdiokWB9gEpm)__ will take a bit more time, but are well laid out.

I would also strongly recommend having a copy of the __[MCMCglmm course notes](https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf)__ open and ready to refer back to whenever something new comes up. They'll explain concepts statistically, whereas I may gloss over some things in order to try making things a little easier when learning about them _for the first time_. 

### Tutorial Aims:

#### <a href="#metaanalysis"> 1. Understanding what a meta-analysis is</a>
#### <a href="#mcmcglmm"> 2. Understanding what MCMCglmm is and why you might want to use it</a>
#### <a href="#effects"> 3. Learning the difference between fixed and random effects (hint: this is different to frequentist statistics)</a>
#### <a href="#syntax"> 4. Becoming familiar with the syntax and model output</a>
#### <a href="#priors"> 5.	Learning what a prior is, and the (absolute) basics on how they work</a>
#### <a href="#parameter"> 6. Understanding parameter expanded priors and measurement error</a>
#### <a href="#extras"> 7. Extras: fixed effects, BLUPS, non-gaussian families, (co)variance structures</a>

<a name="metaanalysis"></a>

## 1. Understanding what a meta-analysis is

#### A meta-analysis is a statistical analysis of results from many individual studies on similar subjects. It provides a much more robust estimate than each individual study alone. It can also reveal patterns and trends across studies, as it allows them to be compared while controlling for sources of *non-independence* and *measurement error* inherent in individual studies.

Comparing studies from different **locations** (e.g. latitude, elevation, hemisphere, climate zone), across **different species** (e.g. with different behaviours or life history traits) or **time periods** (e.g. when the study was done and how long it lasted) introduce sources of *non-independence* which need to be controlled for when estimating an average effect across all studies. 

However, these sources of non-independence may also explain variance around the average effect that we are interested in; for example, perhaps in controlling for latitude, we also discover it explains a large proportion of the variance across studies in the response we are looking at. We can then say that latitude is a good predictor of this response. 

#### As biologists, we are often looking for predictors (such as the locational differences, species, or time periods mentioned above) of how organisms respond to different treatments, or environments etc. 

#### Using a meta-analysis is a great way to do this.

Often results used in a meta-analysis have come from previously published studies. This workshop is aimed at teaching you __what to do once you have collected your data__. 

To learn more about how to conduct a systematic review (the pre-cursor to a meta-analysis) check out this: __[http://www.prisma-statement.org/](http://www.prisma-statement.org)__.  Before continuing with your own meta-analysis, __[this paper](https://doi.org/10.1186/s12915-017-0357-7)__ offers some good advice on what makes a good one.

For now, let’s move on to the next step…

<a name="mcmcglmm"></a>

## 2. Understanding what MCMCglmm is and why you might want to use it

#### MCMCglmm fits _Generalised Linear Mixed-effects Models_ using a _Markov chain Monte Carlo_ approach under a _Bayesian statistical framework_. If some or all of those things make no sense to you, don’t worry – you can still use MCMCglmm without understanding all of this. 

Bayesian statistics sounds scary, but actually it’s more intuitive to understand (in my opinion) than frequentist statistics. 

Using __frequentist statistics__, we calculate the probability of an outcome or hypothesis being true, given the data we have. The more times we repeat an experiment, the more likely we are to estimate the “true” response. The model parameters we use to estimate this response remain fixed, and it is the data that vary, given the number of times the experiment is repeated.

With __Bayesian statistics__, we include prior probabilities in our models, based on our knowledge of previous situations. In this case, the data are fixed and the parameters are what we change, depending on our prior knowledge, and whether we think it's likely that a certain outcome will happen. This is a good schematic of Bayes' theorem. The output of a MCMCglmm model is a __posterior distribution__, which is a combination of your data, your prior knowledge, and the likelihood function.

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc1Bayes.PNG" alt="Img" style="width: 800px;"/> </center>

More info on __GLMMs__ in __[this paper](https://www.sciencedirect.com/science/article/pii/S0169534709000196)__ . __MCMC__ is a bit more difficult to explain. The first __MC__ (__[Markov chain](https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/)__) describes how the model stochastically samples from a target (posterior) distribution given the specified parameters (fixed and random effects and priors etc.). We're going to talk a lot about sampling from a posterior distribution, and it's this part of the package that does this.
The second __MC__ (__[Monte Carlo](https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/)__) describes very complicated integration algorithms used to derive the posterior distribution... you don't need to understand this at all to use MCMCglmm, but feel free to learn a bit more using the above links, and __[this one](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)__. 

Today we are going to focus on using MCMCglmm for __meta-analysis__. The two key reasons why MCMCglmm is so good for this is because as well as controlling for sources of _non-independence_ you can include __phylogeny__, and also __measurement error__. 


<a name="effects"></a>

## 3. Learning the difference between fixed and random effects (hint: this is different to frequentist statistics)

#### In frequentist statistics, we are taught that _fixed effects_ are the variables that we think might have a direct effect on the response variable and will potentially be seen across the whole population (like whether ocean basin or hemisphere affects timing of breeding), and _random effects_ control for things like differences in sampling (e.g. which observer collected the data, the date the samples were collected). I.e. fixed effects are things we are interested in testing, and random effects are things we control for, but don’t necessarily want to know more about.

With __Bayesian statistics__, things are a little bit different. Prepare to have a whole new love of random effects - in many cases, it’s the random effects that we are more interested in, and sometimes we will run whole models with ___no fixed effects at all___ (except the intercept)!

The key is in understanding how MCMCglmm estimates the ___variance___ for each type of effect. 

__Fixed effect models__ assume that there is a single true response for every variable, and will estimate an average effect size for each level of your fixed effect(s). Any variation around this is due to sampling error alone. This type of model is particularly useful in experimental studies. For example, ___do males or females respond more effectively to a treatment___? In a meta-analysis of lots of treatments of individuals, we may include sex as a __fixed effect__ because we want a definitive answer to this question. 

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc2fixed.png" alt="Img" style="width: 800px;"/> </center>

In this funnel plot, each data point represents a response from a previously published study. We use 1/SE as a measure of precision, and to weight each study in the plot. Data points estimated with high standard error will have low precision, and gather towards the bottom of the plot. Vice versa with those estimated with low standard error. Thus, points with low standard error should funnel in around the true effect size.

__In random effects models__, the assumption is different. Instead of estimating a single average effect size (e.g. males respond x amount and females respond y amount), random effects models assume there will be true variation around the average, and the sources of this variation can be explained using the random effects. I.e. there may be variance within males, based on their location, habitat, species etc.

This is particularly useful when asking questions with data from wild systems where we would assume there is lots of natural variation, and we want to find patterns that might help us explain it.

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc3random.png" alt="Img" style="width: 800px;"/> </center>

For example, let’s say we want to know whether migratory birds arrive at their breeding grounds at roughly the same time of year as they did in past decades, or if they now arrive earlier or later as a result of changing climates. We have collected data from many different published studies reporting the number of days earlier or later birds arrive in days/year and days/degree Celsius. This global meta-dataset includes data from many different species, countries, latitudes etc. 

We'll be using a dataset that contains this information. Download the dataset 'migration_metadata.csv' and import it into R to have a look. The data come from this __[meta-analysis of changes in timing of bird migration](http://onlinelibrary.wiley.com/doi/10.1111/1365-2656.12612/full)__.

Using a ___random effects model___ we can calculate the average number of days’ difference across all populations for which we have data using the intercept as our only fixed effect. But, we would assume that there will be a huge amount of variation around this average response, and we want to figure out if we can find any patterns in it. A lot of variance in the observed response across individual studiefs can probably be explained by the fact that data from many different species are included in the dataset. Including species as a random effect will tell us how much variance there is between species, but it won’t estimate an average response for each one. To this effect, you might choose to include something as a random effect when there are __lots of levels__ (in this case there are lots of species) in your variable.

<a name="syntax"></a>

## 4. Becoming familiar with the syntax and model output

To get started, download the data, import it into R and load packages. 

```
### Load packages
library("MCMCglmm") # for meta-analysis
library("dplyr") # for data manipulation


### Read in data
migrationdata <- read.csv(~"migration_metadata.csv", header = T) # import dataset
View(migrationdata) 
# have a look at the dataset. Check out the Predictor variable. There are two, time and temperature.
```

Have a look at the dataset. Check out the Predictor variable. There are two, time and temperature. For the first part of the tutorial, let’s just look at rows where ___annual arrival date has been measured over time___. 

```
migrationdata %>%
filter(Predictor == "year") -> migrationtime # this reduces the dataset to one predictor variable, time.
```

Before we start, let’s plot the data. A __funnel plot__ is typically used to visualize data for meta-analyses. This is done by plotting the predictor variable against 1/standard error for each data point. This weights each study in the plot by its precision, ultimately giving less statistical weight to studies with high standard error. In this case, ‘Slope’ is change in arrival date in days/year.

```
plot(migrationtime$Slope, I(1/migrationtime$SE)) # this makes the funnel plot.

```
<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 3.PNG" alt="Img" style="width: 800px;"/> </center>

You can see here that the data seem to funnel in around zero, and that both positive and negative values are well represented, i.e. this study does not suffer from __publication bias__.
Let’s look at the plot again, with a more zoomed in view. 

```
plot(migrationtime$Slope, I(1/migrationtime$SE), xlim = c(-2,2), ylim = c(0, 60))
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 4.PNG" alt="Img" style="width: 800px;"/> </center>

Now, we can see in more detail that the true value seems to funnel in just left of zero, and there is quite a lot of variation around this. __Understanding how the data look is a good place to start.__

Now we’ll run a ___random effects model___, with only the intercept as a fixed effect. The intercept is going to estimate the average change in arrival date across all data points. There are lots of different species, locations and studies in this analysis, and so the random effects are going to estimate whether there is true variation around the intercept and how much of it can be explained by an effect of species, location, or study. 

```
randomtest <- MCMCglmm (Slope~1, random = ~Species+Location+Study, data = migrationtime)
summary(randomtest)
```

We now have a distribution of estimated outcomes, because MCMCglmm has run through 13,000 iterations of the model and sampled 1000 of them to provide a __posterior distribution__. 

Let's look at our summary statistics, 'summary(randomtest)'. The summary shows us a _posterior mean_ for each effect, _upper_ and _lower 95% Credible Intervals_ (not Confidence Intervals) of the distribution, _effect sample size_ and for the fixed effects, a _pMCMC value_. 

Your effect sample size should be quite high, particularly for more complicated models. 

### Assessing significance

We can accept that a __fixed effect__ is significant when the credible intervals __do not span zero__, this is because if the posterior distribution spans zero, we cannot be confident __that it is not zero__. Ideally your posterior distribution will also be well estimated. Here's an example of a poorly and a well estimated posterior distribution. The red line represents the posterior mean in both cases. 

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 6.PNG" alt="Img" style="width: 800px;"/> </center>

As __random effects__ estimate variance, and variance cannot be zero or negative, we accept that a random effect is significant when the distribution is not pushed up against zero. To check this, we can plot the histogram of each posterior distribution.

```
# Plot the posterior distribution as a histogram to check for significance and whether it's been well estimated or not
# Variance cannot be zero, and therefore if the mean value is pushed up against zero your effect is not significant
# The larger the spread of the histogram, the less well estimated the distribution is.

par(mfrow = c(1,3))

hist(mcmc(randomtest$VCV)[,"Study"])
hist(mcmc(randomtest$VCV)[,"Location"])
hist(mcmc(randomtest$VCV)[,"Species"])
```

center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 7.PNG" alt="Img" style="width: 800px;"/> </center>

Here we can see that the distribution for each effect is pressed right up against zero. For a random effect to be significant, we want the tails to be well removed from zero.

### Assessing model convergence

Now let’s check for model convergence and see whether we have used the correct parameters for the model. We do this separately for both fixed and random effects. 

```
plot(randomtest$Sol)

```
<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 8.PNG" alt="Img" style="width: 800px;"/> </center>

Here you can see the trace and density estimate for the intercept. The trace is like a time series of what your model did while it was running and can be used to assess mixing (or convergence), while the density is like a smoothed histogram of the estimates that the model produced for every iteration of the model.

Each iteration of your model should be independent of the previous one, thanks to the Markov chain i.e. it should not be autocorrelated.

To make sure your model has run correctly, the trace plot should look like a fuzzy caterpillar - this way you know it is not autocorrelated. It looks like the intercept has mixed well. 

If you suspect autocorrelation there are a few things you can do.

__1)__ Increase the number of iterations, default is 13000 (e.g. nitt = 60000, I often use hundreds of thousands of iterations for more complex models)

__2)__ Increase the "burn in", the default here is that MCMCglmm will discount the first 3000 iterations which aren't as accurate as the model hasn't converged yet, you can increase this (e.g. burnin = 5000)

__3)__ Think about using a stronger prior, but more on that in a little while. 

For more on diagnostics check this link out: __[http://sbfnk.github.io/mfiidd/mcmc_diagnostics.html](http://sbfnk.github.io/mfiidd/mcmc_diagnostics.html)__.


Let’s do the same thing now, but for the random effects:

```
plot(randomtest$VCV)
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 9.PNG" alt="Img" style="width: 800px;"/> </center>

It looks like some of the random effects haven’t mixed very well at all. The effect sample size is also very small. Maybe we could improve this by increasing the number of iterations, but because the chain seems to be stuck around zero, it looks like ___we’ll need to use a stronger prior___ than the default.

Chapter 8 of the MCMCglmm course notes will be a good thing to read at this point.


<a name="priors"></a>

## 5. Learning what a prior is, and the (absolute) basics on how they work</a>

The most difficult part of a Bayesian analysis to understand is how to fit __correct priors__. 

These are mathematical quantifications of prior knowledge, and we fit a separate prior for each fixed and random effect, and for the residual.

We can thus use priors to inform the model which shape we think the posterior distribution will take. 
In the schematic below, you can see we use our prior beliefs to “drag” the distribution of our evidence (our collected data) towards the left. 

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc4priorposterior.png" alt="Img" style="width: 800px;"/> </center>

It’s very difficult to understand how the prior interacts with the distribution of the data to give the posterior distribution, unless you’re a mathematician. 

Perhaps fortunately for us, we’re biologists, and that’s why we need complex algorithms like MCMC. However, it’s very difficult to be confident that you have done this correctly, and a key reason why Bayesian statistics can be confusing.

Firstly, priors can vary in how informative they are. __Weakly informative__ priors should be used in situations where we don’t have much prior knowledge and want the data to speak for themselves. The prior won’t drag the posterior distribution away from the actual distribution of the data _too much_. __Informative__ priors provide information that is crucial to the estimation of the model, and will shape the posterior distribution quite a bit. While each prior follows a similar formula, whether it is strongly or weakly informative depends on the values you include in it. 

Be careful when you read that a prior is __uninformative__; there is no such thing as a completely uninformative prior.

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc5priorstrength.png" alt="Img" style="width: 800px;"/> </center>

With MCMCglmm, the default prior assumes a normal posterior distribution for the fixed effects (weakly informative) and an inverse Wishart prior for the random effects ((co) variances). Below you can see what an inverse Wishart prior looks like in graphical terms. You can see that nu can vary in its strength and level of information. Imagine what each level of nu might do to your data - we might expect that when nu is low, it will be less informative?

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/mcmc6nu.PNG" alt="Img" style="width: 800px;"/> </center>

The more complicated your models become, the more likely it is that you will eventually get an error message, or as we have just seen, that your models will not mix from the beginning. In this case we should use __parameter expanded priors__ of our own. They are less easy to specify incorrectly than inverse Wishart priors. 

__However, proceed with caution from this point on!__

<a name="parameter"></a>

## 6. An introduction to parameter expanded priors and measurement error 

#### Let’s run the model again, but this time we’ll use __parameter expanded priors__ for the random effects by including ```prior = prior1```. Each random effect is represented by a G, and the residual is represented by R. The parameter expansion refers to the fact that we have included a prior mean (alpha.mu) and (co)variance matrix (alpha.V). You can read more about this in the course notes, and learn about (co)variance matrices too. 

```
a <- 1000
prior1<-list(R=list(V=diag(1),nu=0.002)
             , G=list(G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)))

randomprior <- MCMCglmm (Slope~1, random = ~Species+Location+Study, data = migrationtime, prior=prior1, nitt = 60000)
```

Note that I have increased the number of iterations as 60000 to improve mixing and effect sample sizes.

```
summary(randomprior)
```

The effect sample sizes are much bigger now! This is a good sign.

```
plot(randomprior$VCV)
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 12.PNG" alt="Img" style="width: 800px;"/> </center>

The models look to have mixed much better too. This is also good. 

Before we do our model checks, I want to include __measurement error__ by including ```r idh(SE):units ``` as a random effect. This is one of the key reasons we would use MCMCglmm for meta-analysis, and you can read the meta-analysis section of the course notes to understand more. 

A key assumption of a meta-analysis is that the variance between the standard errors for each data point is fixed at 1, so we’ll adjust our random effects and priors accordingly. You can see that we now have four random priors, the last of which is fixed at 1.

```
prior2<-list(R=list(V=diag(1),nu=0.002)
             , G=list(G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1),fix=1)))

randomerror2 <- MCMCglmm (Slope~1, random = ~Species+Location+Study+idh(SE):units, data = migrationtime, prior=prior2, nitt = 60000)

plot(randomerror2$VCV)
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 14.PNG" alt="Img" style="width: 800px;"/> </center>

If you check the summary now, you can see that now we’ve included measurement error, our estimates are much more conserved. Studies with higher standard error have been given lower statistical weight.

Now, we’ll perform our model checks. To do this, we will simulate new data given the same variance/co-variance structures (priors), and then plot it against our real data to make sure they overlap. 

```
xsim <- simulate(randomerror2) # reruns 100 new models, based around the same variance/covariance structures but with simulated data.

plot(migrationtime$Slope, I(1/migrationtime$SE))
points(xsim, I(1/migrationtime$SE), col = "red") # here you can plot the data from both your simulated and real datasets and compare them
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 15.PNG" alt="Img" style="width: 800px;"/> </center>

This seems to fit reasonably well, although the simulated data are perhaps skewed a bit too much towards the left. This may happen in meta-analyses where more studies than expected have high standard errors, thus causing the variance to be higher than 1.

Let's rerun the model, but this time changing the prior for measurement error so that it is no longer fixed at 1.

```
prior3<-list(R=list(V=diag(1),nu=0.002)
             , G=list(G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)))

randomerror3 <- MCMCglmm (Slope~1, random = ~Species+Location+Study+idh(SE):units, data = migrationtime, prior=prior3, nitt = 60000)
```

Now we can simulate new data again, and plot it against our collected data. 

```
xsim<-simulate(randomerror3)

plot(migrationtime$Slope, I(1/migrationtime$SE))
points(xsim, I(1/migrationtime$SE), col = "red") # here you can plot the data from both your simulated and real datasets and compare them
```

<center> <img src="{{ site.baseurl }}/img/mcmcglmm/Image 17.PNG" alt="Img" style="width: 800px;"/> </center>

These parameters seem to be a better fit for our data. 

Another trick for checking whether the parameters fit the data is to make sure your max (or min) values fit inside a histogram of your simulated distribution, like this:

```
xsim<-simulate(randomerror3, 1000) # 1000 represents the number of simulations, and for some reason needs to be higher than 
hist(apply(xsim, 2, max), breaks = 30)

abline(v = max(migration$Slope), col = "red")
```

However, there is still some residual variance left over and so we might include other random effects as we see fit. Hopefully this has given you a good idea on how to get started with MCMCglmm!

<a name="extras"></a>

## 7. Extras: fixed effects, BLUPS, non-gaussian families, (co)variance structures

### Fixed effects

As well as random effects, you can also fit fixed effects, just like in a regular model. Remember however, that MCMCglmm will estimate a __true average effect size__ for your fixed effects, rather than the variance. 

By including Continent as a __fixed effect__ in the model below, we are asking _what is the average change in arrival date on each continent?_ If we included it as a __random effect__ we would be asking _how much do arrival dates vary across continents?_

```
fixedtest <- MCMCglmm (Slope~Migration_distance+Continent, random = ~Species+Location+Study, data = migrationtime, mev = sqrd.se, nitt = 60000)
```

__Note:__ I have never had to change the priors for a fixed effect, but this would be worth some research for your own projects.

### Best Linear Unbiased Predictors for random effects

I previously mentioned that MCMCglmm only estimates the variance within a random effect, but not a true effect size for each level. E.g. species may be a good predictor of variance in changes in arrival date, but we don't know _which_ species arrive the earliest/latest. This is not completely true. __<a href="https://en.wikipedia.org/wiki/Best_linear_unbiased_prediction" target ="_blank">BLUPs</a>__ are a useful tool for __predicting the effect size of your random effects__.

There is a key difference between this and fixed effects though. With fixed effects we are _estimating_ an effect and can be confident that it is reliable, and with random effects we are _predicting_ it. This is because the equations used to calculate both are different. You should therefore never do any further statistical analyses on a BLUP, and always be sure to let your reader know that this is where your prediction has come from. 

To save the BLUPs for each random effect we use ```pr = TRUE```, which weirdly saves them in the ```$Sol``` part of the model output. Take a look:

```fixedtest <- MCMCglmm (Slope~Migration_distance+Continent, random = ~Species+Location+Study, data = migrationtime, mev = sqrd.se, pr = TRUE, nitt = 60000)```

Each BLUP has a posterior distribution, and so we can calculate the mean for each.

```randomblup <- apply(fixedtest$Sol,2,mean)```

If we want to look at the BLUPs for species, for example, we can choose the correct rows using the code below. This is a bit fiddly if you have lots of levels.

```
names(randomblup)
randomblup[9:416]
```

This code will sort from smallest value to largest.

```sort(randomblup[9:416])```

### Non-gaussian families 

This tutorial has been based around using a Gaussian distribution. However, MCMCglmm can handle non-Gaussian families as well. Specify ```family=``` to choose the correct distribution.

### (Co)variance structures

Until now, we have learned that for each random effect and the residual in our model, MCMCglmm estimates the variance within that effect, i.e. the variance is in a 1x1 matrix - [_V_]. However, we can __restructure the variance matrix__ within random effects and the residual if we want. 

Here's an example. In previous models, we have assumed that all measures of arrival date were the same, but actually, it has been measured in three different ways; first, mean and median. Peak arrival is included here too, but there are no rows containing it.

```levels(migrationtime$Response_variable)```

Our residual variance is therefore heterogeneous, and we need to take this into account in our model. We can do this by using the ```idh():units``` function in ```rcov```.

We have to change the variance structure for the residual prior. In this case, we use a 3x3 variance matrix, because there are three types of response.

```prior4<-list(R=list(V=diag(3),nu=0.002)
             , G=list(G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)))
```

If you just check out ```prior4``` you should be able to visualise the matrix for the residual prior a bit more easily.

```fixedtest <- MCMCglmm (Slope~Migration_distance+Continent, random = ~Species+Location+Study, data = migrationtime, rcov = ~idh(Response_variable):units, mev = sqrd.se, pr = TRUE, nitt = 60000)

summary(fixedtest)```

Now we can see when we print the summary that the residual variance has now been estimated for all three measures of arrival. Success!

__Note:__ you can also restructure the variance for your random effects and in this case you will need to update the effect's prior. Here is an example if we wanted to estimate the variance for each location for which we have data. There are 25 different locations:


```prior5<-list(R=list(V=diag(3),nu=0.002)
             , G=list(G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)
                      , G1=list(V=diag(25), nu=25, alpha.mu=rep(0,25), alpha.V=diag(25)*a)
                      , G1=list(V=diag(1), nu=1, alpha.mu=0, alpha.V=diag(1)*a)))
 prior5
```

```fixedtest <- MCMCglmm (Slope~Migration_distance+Continent, random = ~Species+idh(Location)+Study, data = migrationtime, rcov = ~idh(Response_variable):units, mev = sqrd.se, pr = TRUE, nitt = 60000)```

__Finally__, as well as using variance matrices, you can use co-variance matrices, replacing ```idh()``` with ```us()```. However, I think this concept falls beyond the remit of an introduction to MCMCglmm. You can learn more about how to fit these matrices, and get some nice visualisations of what the matrices for each functionlook like in the "Compound Variance Structures" section of the Course Notes. Happy structuring!